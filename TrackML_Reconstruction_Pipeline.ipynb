{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75258fb7",
   "metadata": {},
   "source": [
    "# TrackML Particle Tracking \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8c6d39",
   "metadata": {},
   "source": [
    "**Dataset used: https://www.kaggle.com/c/trackml-particle-identification.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d386a1",
   "metadata": {},
   "source": [
    "The Large Hadron Collider (LHC) is the world's largest and most powerful particle accelerator, located at CERN (European Organization for Nuclear Research) on the Franco-Swiss border. In this project, we analyze data produced from the collisions by looking at the information from silicon detectors. The objective of this work is to identify the original tracks of the particles by joining the 3D coordinates of the hits.\n",
    "\n",
    "With this aim, we will train a GNN by identifying the hits with the nodes and the edges with the conexions between nodes. Our GNN model must predict which is the real track of the particle, by selecting the correct edges between hits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd40107d",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911ee176",
   "metadata": {},
   "source": [
    "We can use this library to use our dataset more easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbe3784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/LAL/trackml-library.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed130d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "# Use GPU if available (Essential for GNN training)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Running on: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00afb6d3",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021f97ff",
   "metadata": {},
   "source": [
    "We'll start by taking only 5000 hits of one event, so that we can run it on the cpu. We focus on the Pixel Tracker to reduce the compilation time, and filter the hits that occur in the proximities of the center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e704641",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trackml.dataset import load_event\n",
    "\n",
    "hits, cells, particles, truth = load_event('train_sample/train_100_events/event000001000')\n",
    "\n",
    "# Join hits with truth data to identify which hit belongs to which particle\n",
    "hits = hits.merge(truth[['hit_id', 'particle_id']], on='hit_id')\n",
    "\n",
    "# We filter for the Pixel Tracker (the volumes of interest)\n",
    "# These volumes (8, 13, 17) represent the innermost layers of the detector.\n",
    "\n",
    "hits = hits[hits.volume_id.isin([8, 13, 17])]\n",
    "\n",
    "# Next, we take a manageable sample:\n",
    "\n",
    "hits = hits[\n",
    "    (hits.volume_id.isin([8, 13, 17])) & \n",
    "    (hits.z > -50) & (hits.z < 50)\n",
    "].copy()\n",
    "\n",
    "\n",
    "print(hits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71455abf",
   "metadata": {},
   "source": [
    "We have considerably reduced the number of instances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c4be5e",
   "metadata": {},
   "source": [
    "# Defining the GNN Architecture (Interaction Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1791f1bc",
   "metadata": {},
   "source": [
    "##  Cylindrical coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183e7e82",
   "metadata": {},
   "source": [
    "The $(x, y, z)$ coordinates represent the exact spatial position of a \"hit\" (impact) within the detector, measured with micrometric precision, by using a cartesian coordinate system with:\n",
    "- Origin $(0, 0, 0)$: This is the geometric center of the detector, where the proton beams collide.Z-axis: Extends along the beam pipe (where protons travel before colliding). Particles traveling \"forward\" or \"backward\" have very large $z$ values (positive or negative).\n",
    "- XY Transverse Plane: This is the plane perpendicular to the beam. This is where CERN's magnetic field curves particle trajectories so we can measure their momentum.Units: All $(x, y, z)$ coordinates are expressed in millimeters (mm).\n",
    "\n",
    "For tracking algorithms and your GNN, we often convert these Cartesian coordinates into cylindrical ones ($r, \\phi, z$), as the detector has a cylindrical symmetry:\n",
    "- Radius ($r$): Calculated as $r = \\sqrt{x^2 + y^2}$. It indicates how far from the center the particle has reached.\n",
    "- Azimuthal Angle ($\\phi$): Calculated as $\\phi = \\arctan2(y, x)$. It indicates the particle's direction in the detector's circle.\n",
    "- Z: Remains the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e902ce83",
   "metadata": {},
   "source": [
    "## Graph Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fc0653",
   "metadata": {},
   "source": [
    "\n",
    "Our goal is to build an Interaction Network. This model will perform Edge Classification: it looks at a connection between two hits and predicts a value between 0 and 1.\n",
    "\n",
    "- Target 1: True segment (both hits belong to the same particle_id).\n",
    "\n",
    "- Target 0: Fake segment (noise or different particles).\n",
    "\n",
    "However, to build a graph that is actually usable for a GNN, we cannot simply connect every hit to every other hit. This would result in too many edges, which is physically impossible to compute. To reduce the number of edges, we add some constraints based on the laws of physics:\n",
    "\n",
    "- **Layer-to-Layer Constraint:** It's the most obvious one. Particles are created at the center and travel outward. A hit in an inner layer can only be logically followed by a hit in an outer layer. By enforcing this, we eliminate millions connections that make no sense physically.\n",
    "\n",
    "- **Angular Difference ($\\Delta \\phi$):** The LHC uses a powerful magnetic field to curve the paths of charged particles. However, high-energy particles (the ones we care about) have a high momentum and do not curve sharply. Their angular direction ($\\phi$) changes very little between adjacent layers. \n",
    "\n",
    "- **Z-Distance ($\\Delta z$):** Since the particles originate from the beam-collision point at the center, their trajectory in the $Z$ direction (along the beam pipe) is relatively straight. If two hits have a massive vertical distance ($\\Delta z$) but are close in the $XY$ plane (which may happen if they are in adjacent layers), it is highly unlikely that they belong to the same track.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b16d63",
   "metadata": {},
   "source": [
    "Let's build a function that computes the nodes of our model, taking in consideration the previous constraints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a42f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def build_graph(hits, delta_phi_max=0.5, delta_z_max=20):\n",
    "   \n",
    "    hits['r'] = np.sqrt(hits.x**2 + hits.y**2)\n",
    "    hits['phi'] = np.arctan2(hits.y, hits.x)\n",
    "    \n",
    "    # Create a unique layer identifier (Global Layer)\n",
    "    # Combining volume_id and layer_id to distinguish layers across different volumes\n",
    "    hits['global_layer'] = hits.volume_id * 100 + hits.layer_id\n",
    "    \n",
    "    # Get a sorted list of all unique global layers present in the data\n",
    "    ordered_layers = np.sort(hits.global_layer.unique())\n",
    "    \n",
    "    # Normalization\n",
    "    # Instead of /1000, it's better to have values ​​close to 0 with a mean of 1\n",
    "    x_features = torch.tensor(hits[['x', 'y', 'z', 'r', 'phi']].values, dtype=torch.float)\n",
    "    x_features[:, :3] /= 1000.0\n",
    "    x_features[:, 3] /= 1000.0 \n",
    "    x_features[:, 4] /= np.pi  \n",
    "    \n",
    "    # Extract coordinate arrays for faster vectorized filtering\n",
    "    phi = np.arctan2(hits.y.values, hits.x.values)\n",
    "    z = hits.z.values\n",
    "    layer_array = hits.global_layer.values\n",
    "    particle_ids = hits.particle_id.values\n",
    "    \n",
    "    senders, receivers, y = [], [], []\n",
    "\n",
    "    # Iterate layer by layer (except the last one) to find adjacent connections\n",
    "    for layer_idx in range(len(ordered_layers) - 1):\n",
    "        current_layer = ordered_layers[layer_idx]\n",
    "        next_layer = ordered_layers[layer_idx + 1]\n",
    "        \n",
    "        # Get indices of hits belonging to the current and the next adjacent layer\n",
    "        current_indices = np.where(layer_array == current_layer)[0]\n",
    "        next_indices = np.where(layer_array == next_layer)[0]\n",
    "        \n",
    "        for i in current_indices:\n",
    "            # Filter candidates ONLY in the immediately succeeding layer\n",
    "\n",
    "            d_phi = np.abs(phi[next_indices] - phi[i])\n",
    "            d_phi = np.where(d_phi > np.pi, 2*np.pi - d_phi, d_phi)   # Periodicity Management\n",
    "\n",
    "            potential_neighbors = next_indices[(d_phi < delta_phi_max) & (np.abs(z[next_indices] - z[i]) < delta_z_max)]\n",
    "            \n",
    "            for j in potential_neighbors:\n",
    "                senders.append(i)\n",
    "                receivers.append(j)\n",
    "                \n",
    "                # Assign Truth Label: 1 if both hits belong to the same particle (and not noise)\n",
    "                is_same_particle = (particle_ids[i] == particle_ids[j]) and (particle_ids[i] != 0)\n",
    "                y.append(1 if is_same_particle else 0)\n",
    "\n",
    "    # Convert edge lists to PyTorch Tensors\n",
    "    edge_index = torch.tensor([senders, receivers], dtype=torch.long)\n",
    "    edge_label = torch.tensor(y, dtype=torch.float)\n",
    "    \n",
    "    # Return the PyTorch Geometric Data object\n",
    "    return Data(x=x_features, edge_index=edge_index, y=edge_label)\n",
    "\n",
    "# Process the first event\n",
    "graph_data = build_graph(hits)\n",
    "print(f\"Graph created with {graph_data.num_nodes} nodes and {graph_data.num_edges} edges.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d973083",
   "metadata": {},
   "source": [
    "## Graph visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8408edc6",
   "metadata": {},
   "source": [
    "First, we visualize both true and false edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341b0354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "def plot_graph_3d(data, n_edges=500):\n",
    "\n",
    "    # We count the number of false and true edges represented\n",
    "    number_false_edges=0\n",
    "    number_true_edges=0\n",
    "\n",
    "    # Extract Node Positions (Rescale back to mm)\n",
    "    pos = data.x.cpu().numpy() * 1000.0 \n",
    "    \n",
    "    # Create the Hit (Node) Scatter Plot\n",
    "    node_trace = go.Scatter3d(\n",
    "        x=pos[:, 0], y=pos[:, 1], z=pos[:, 2],\n",
    "        mode='markers',\n",
    "        marker=dict(size=2, color='blue', opacity=0.8),\n",
    "        name='Hits'\n",
    "    )\n",
    "\n",
    "    # Create Edge Traces\n",
    "    # We only plot a subset of edges to avoid crashing the browser\n",
    "    edge_index = data.edge_index.cpu().numpy()\n",
    "    edge_y = data.y.cpu().numpy()\n",
    "    \n",
    "    # Separate True and Fake edges for coloring\n",
    "    edge_x, edge_y_coords, edge_z = [], [], []\n",
    "    true_edge_x, true_edge_y, true_edge_z = [], [], []\n",
    "\n",
    "    for i in range(min(n_edges, data.num_edges)):\n",
    "        start_node = edge_index[0, i]\n",
    "        end_node = edge_index[1, i]\n",
    "        \n",
    "        # Segment coordinates\n",
    "        x_coords = [pos[start_node, 0], pos[end_node, 0], None]\n",
    "        y_coords = [pos[start_node, 1], pos[end_node, 1], None]\n",
    "        z_coords = [pos[start_node, 2], pos[end_node, 2], None]\n",
    "        \n",
    "        if edge_y[i] == 1:\n",
    "            true_edge_x.extend(x_coords); true_edge_y.extend(y_coords); true_edge_z.extend(z_coords)\n",
    "            number_true_edges+=1\n",
    "        else:\n",
    "            edge_x.extend(x_coords); edge_y_coords.extend(y_coords); edge_z.extend(z_coords)\n",
    "            number_false_edges+=1\n",
    "\n",
    "    # Fake edges (Red)\n",
    "    fake_trace = go.Scatter3d(\n",
    "        x=edge_x, y=edge_y_coords, z=edge_z,\n",
    "        mode='lines', line=dict(color='red', width=1),\n",
    "        name='Fake Edges (Proposed)', opacity=0.3\n",
    "    )\n",
    "    \n",
    "    # True edges (Green)\n",
    "    true_trace = go.Scatter3d(\n",
    "        x=true_edge_x, y=true_edge_y, z=true_edge_z,\n",
    "        mode='lines', line=dict(color='green', width=3),\n",
    "        name='True Edges (Signal)'\n",
    "    )\n",
    "\n",
    "    # Final Layout\n",
    "    fig = go.Figure(data=[node_trace, fake_trace, true_trace])\n",
    "    fig.update_layout(\n",
    "        title=\"3D Graph Representation: Internal Pixel Tracker\",\n",
    "        scene=dict(xaxis_title='X (mm)', yaxis_title='Y (mm)', zaxis_title='Z (mm)', aspectmode='data'),\n",
    "        margin=dict(l=0, r=0, b=0, t=40)\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "    print(f\"The representation contains {number_true_edges} true edges and {number_false_edges} false edges\")\n",
    "\n",
    "# Visualize the first edges of your graph\n",
    "plot_graph_3d(graph_data, n_edges=617136)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43fcac7",
   "metadata": {},
   "source": [
    "To get a more clear visualization, we can plot only the true tracks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33f48f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "def plot_true_edges_3d(data, n_edges=500):\n",
    "    \"\"\"\n",
    "    Plots a 3D representation of the hits and ONLY the true (signal) edges.\n",
    "    \"\"\"\n",
    "    # Counter for true edges found in the sampled range\n",
    "    number_true_edges = 0\n",
    "\n",
    "    # Extract Node Positions (Rescale back to mm)\n",
    "    # Assuming features are normalized, we scale by 1000 for mm units\n",
    "    pos = data.x.cpu().numpy() * 1000.0 \n",
    "    \n",
    "    # Create the Hit (Node) Scatter Plot\n",
    "    node_trace = go.Scatter3d(\n",
    "        x=pos[:, 0], y=pos[:, 1], z=pos[:, 2],\n",
    "        mode='markers',\n",
    "        marker=dict(size=2, color='blue', opacity=0.8),\n",
    "        name='Hits (Detector Hits)'\n",
    "    )\n",
    "\n",
    "    # Create True Edge Trace\n",
    "    edge_index = data.edge_index.cpu().numpy()\n",
    "    edge_y = data.y.cpu().numpy()\n",
    "    \n",
    "    # Lists to store coordinates for true edges\n",
    "    true_edge_x, true_edge_y, true_edge_z = [], [], []\n",
    "\n",
    "    # Iterate through the edges up to the specified limit\n",
    "    for i in range(min(n_edges, data.num_edges)):\n",
    "        # Only process if it is a TRUE edge (y = 1)\n",
    "        if edge_y[i] == 1:\n",
    "            start_node = edge_index[0, i]\n",
    "            end_node = edge_index[1, i]\n",
    "            \n",
    "            # Append coordinates for the line segment [Start, End, None to break the line]\n",
    "            true_edge_x.extend([pos[start_node, 0], pos[end_node, 0], None])\n",
    "            true_edge_y.extend([pos[start_node, 1], pos[end_node, 1], None])\n",
    "            true_edge_z.extend([pos[start_node, 2], pos[end_node, 2], None])\n",
    "            \n",
    "            number_true_edges += 1\n",
    "\n",
    "    # Define the Green Trace for True Edges\n",
    "    true_trace = go.Scatter3d(\n",
    "        x=true_edge_x, y=true_edge_y, z=true_edge_z,\n",
    "        mode='lines', \n",
    "        line=dict(color='green', width=3),\n",
    "        name='True Edges (Signal Tracks)',\n",
    "        opacity=1.0\n",
    "    )\n",
    "\n",
    "    # Final Layout Configuration\n",
    "    fig = go.Figure(data=[node_trace, true_trace])\n",
    "    fig.update_layout(\n",
    "        title=\"3D Particle Tracking: Ground Truth Signal Edges\",\n",
    "        scene=dict(\n",
    "            xaxis_title='X (mm)', \n",
    "            yaxis_title='Y (mm)', \n",
    "            zaxis_title='Z (mm)',\n",
    "            aspectmode='data' # Maintains physical proportions\n",
    "        ),\n",
    "        margin=dict(l=0, r=0, b=0, t=40)\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "    print(f\"Visualization complete. Represented {number_true_edges} true edges.\")\n",
    "\n",
    "plot_true_edges_3d(graph_data, n_edges=617136)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3404359c",
   "metadata": {},
   "source": [
    "## Model arquitecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3305fb25",
   "metadata": {},
   "source": [
    "We'll start by deciding how the GNN will learn the relationships between nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f7b3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrackNet(MessagePassing):\n",
    "    def __init__(self, node_in=5, edge_in=15, hidden=128):\n",
    "        super(TrackNet, self).__init__(aggr='add')\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(edge_in, hidden),\n",
    "            nn.BatchNorm1d(hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, 1)\n",
    "        )\n",
    "        self.node_mlp = nn.Sequential(\n",
    "            nn.Linear(node_in + 1, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, node_in)\n",
    "        )\n",
    "        self.current_logits = None       # To store the edge scores\n",
    "\n",
    "    def forward(self, x, edge_index):    # Model output \n",
    "        self.propagate(edge_index, x=x)\n",
    "        return self.current_logits.squeeze(-1)\n",
    "\n",
    "    def message(self, x_i, x_j):\n",
    "\n",
    "        delta_pos = x_j[:, :3] - x_i[:, :3]\n",
    "        delta_r = x_j[:, 3:4] - x_i[:, 3:4]\n",
    "        delta_phi = x_j[:, 4:5] - x_i[:, 4:5]        \n",
    "        \n",
    "        edge_input = torch.cat([x_i, x_j, delta_pos, delta_r, delta_phi], dim=1)\n",
    "        \n",
    "        self.current_logits = self.edge_mlp(edge_input)\n",
    "        return self.current_logits\n",
    "    \n",
    "    def update(self, aggr_out, x):\n",
    "        \"\"\"\n",
    "        FINAL STEP OF THE ALGORITHM:\n",
    "        aggr_out: The sum of scores (logits) of all edges touching each hit.\n",
    "        x: The original hit features [x, y, z, r, phi].\n",
    "        \"\"\"\n",
    "        # Squash the summed message so it doesn't \"drown out\" the x coordinates\n",
    "        msg_scaled = torch.sigmoid(aggr_out)\n",
    "        \n",
    "        # Now x (normalized) and msg_scaled (0-1) have the same statistical weight\n",
    "        node_input = torch.cat([x, msg_scaled], dim=1)\n",
    "        \n",
    "        # Concatenate the hit position with the sum of its messages (5 + 1 = 6 columns)\n",
    "        node_input = torch.cat([x, aggr_out], dim=1) \n",
    "        \n",
    "        # The node_mlp processes this union and returns the \"contextualized\" node\n",
    "        return self.node_mlp(node_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc2154a",
   "metadata": {},
   "source": [
    "And we create our model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668e21fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TrackNet().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4b036b",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c23096",
   "metadata": {},
   "source": [
    "First, we can observe that the edge classes are strongly unbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1741047",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pos = (graph_data.y == 1).sum().item()\n",
    "num_neg = (graph_data.y == 0).sum().item()\n",
    "print(f\"Positive edges: {num_pos}, Negative edges: {num_neg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1b3efa",
   "metadata": {},
   "source": [
    "In the training process, we will use Binary Cross Entropy (BCE) because this is a binary classification problem, assigning different weights to each class to take into consideration its imbalance. We use the Adam optimizer for its fast convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae569ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import time\n",
    "\n",
    "# Initialize model\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)        # Adam optimizer\n",
    "\n",
    "weight = torch.tensor([611323 / 5813]).to(device)\n",
    "criterion = torch.nn.BCEWithLogitsLoss(pos_weight=weight)   # BCE\n",
    "\n",
    "# Move data to the same device as the model\n",
    "graph_data = graph_data.to(device)\n",
    "\n",
    "print(f\"Training on: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb5809f",
   "metadata": {},
   "source": [
    "We'll use the metric AUC-ROC for binary classification of edges. First, we'll build a function that computes one complete epoch in our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc2e619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch():\n",
    "    model.train()\n",
    "    optimizer.zero_grad() \n",
    "    \n",
    "    edge_logits = model(graph_data.x, graph_data.edge_index)\n",
    "    \n",
    "    #  We calculate the loss using the model's logits\n",
    "    # 'graph_data.y' are the ground truth labels (1 if it is a real trajectory, 0 if not)\n",
    "    loss = criterion(edge_logits, graph_data.y.float())\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    # Gradient Clipping to prevent gradients from exploding\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    # Return the loss and the scores (logits) to monitor progress\n",
    "    return loss.item(), edge_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2444b8",
   "metadata": {},
   "source": [
    "Now, we can apply it to our data to train for 400 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1146ade2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training for 200 epochs\n",
    "for epoch in range(1, 401):\n",
    "    loss_val, scores = train_one_epoch()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        # Calculate AUC-ROC: A value of 1.0 means perfect classification\n",
    "        auc = roc_auc_score(graph_data.y.cpu().detach(), scores.cpu().detach())\n",
    "        print(f\"Epoch {epoch:03d} | Loss: {loss_val:.4f} | AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6114ad39",
   "metadata": {},
   "source": [
    "We can observe that we get a considerably high AUC (near 0.95)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a99867",
   "metadata": {},
   "source": [
    "## Expected score "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dedb94",
   "metadata": {},
   "source": [
    "We build a function to predict the test edges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f8b6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "@torch.no_grad()   # Disables gradient calculation to save memory\n",
    "def predict_test_edges(model, test_data):\n",
    "    model.eval()   # Sets the model to evaluation mode (fixes BatchNorm/Dropout)\n",
    "    \n",
    "    # Forward pass: \n",
    "    edge_logits = model(test_data.x, test_data.edge_index)\n",
    "     \n",
    "    # Diagnostic \n",
    "    print(f\"Max Logit: {edge_logits.max().item():.4f}\")\n",
    "    print(f\"Min Logit: {edge_logits.min().item():.4f}\")\n",
    "    print(f\"Mean Logits: {edge_logits.mean().item():.4f}\")\n",
    "\n",
    "    # Convert logits to probabilities\n",
    "    # We use the sigmoid function to map from range (-inf, +inf) to range (0, 1)\n",
    "    probs = torch.sigmoid(edge_logits)\n",
    "    \n",
    "    # Move to CPU and NumPy format for the submission dataframe\n",
    "    return probs.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d64402",
   "metadata": {},
   "source": [
    "We'll start by using a _connected components_ solution to reconstruct the tracks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06f0bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "\n",
    "def get_submission_df(test_data, edge_probs, threshold=0.5):\n",
    "    # Filter edges by probability\n",
    "    mask = edge_probs > threshold\n",
    "    rows = test_data.edge_index[0, mask].cpu().numpy()\n",
    "    cols = test_data.edge_index[1, mask].cpu().numpy()\n",
    "    \n",
    "    # Create adjacency matrix and calculate connected components\n",
    "    num_nodes = test_data.x.size(0)\n",
    "    adj = csr_matrix((np.ones(len(rows)), (rows, cols)), shape=(num_nodes, num_nodes))\n",
    "    _, labels = connected_components(csgraph=adj, directed=False)\n",
    "    \n",
    "    # Format required by the TrackML metric\n",
    "    return pd.DataFrame({\n",
    "        'hit_id': np.arange(num_nodes),\n",
    "        'track_id': labels\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403c3e8b",
   "metadata": {},
   "source": [
    "Finally, we can plot the predicted probabilities of the test edges (which are the train ones)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a2c276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "test_data = build_graph(hits)\n",
    "\n",
    "truth_test_df = truth[truth.hit_id.isin(hits.hit_id)].copy()\n",
    "\n",
    "# We get the probabilities \n",
    "probs = predict_test_edges(model, test_data)\n",
    "\n",
    "# We can plot them on a bar plot\n",
    "plt.hist(probs, bins=50)\n",
    "plt.title(\"Distribución de Probabilidades\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57814956",
   "metadata": {},
   "source": [
    "We get the predicted tracks and represent them: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908718ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_tracks = get_submission_df(test_data, probs, threshold=0.8)\n",
    "\n",
    "print(f\"Pistas únicas encontradas: {predicted_tracks['track_id'].nunique()}\")\n",
    "print(f\"Total de hits: {len(predicted_tracks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea0bd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "def plot_final_tracks_3d(test_data, predicted_tracks, num_tracks=30, min_hits=3):\n",
    "    \"\"\"\n",
    "    Visualizes the trajectories reconstructed by the model.\n",
    "    \"\"\"\n",
    "    # Retrieve coordinates (Make sure to multiply by the normalization factor used)\n",
    "    coords = test_data.x[:, :3].cpu().numpy() * 1000.0 \n",
    "    \n",
    "    df_plot = predicted_tracks.copy()\n",
    "    df_plot['x'], df_plot['y'], df_plot['z'] = coords[:, 0], coords[:, 1], coords[:, 2]\n",
    "    \n",
    "    # Calculate Radius (r) to SORT the hits\n",
    "    df_plot['r'] = np.sqrt(df_plot['x']**2 + df_plot['y']**2)\n",
    "    \n",
    "    # Filter tracks for visualization\n",
    "    track_counts = df_plot['track_id'].value_counts()\n",
    "    \n",
    "    # We filter tracks with a realistic number of hits (Pixel Tracker physics: 3-15 hits)\n",
    "    # We avoid plotting a \"super track\" if it exists, as it would hide everything else\n",
    "    valid_tracks = track_counts[(track_counts >= min_hits) & (track_counts < 30)].index.tolist()\n",
    "    \n",
    "    if not valid_tracks:\n",
    "        print(\"No valid tracks were found with the current threshold and filters.\")\n",
    "        return\n",
    "\n",
    "    selected_ids = np.random.choice(valid_tracks, min(num_tracks, len(valid_tracks)), replace=False)\n",
    "    \n",
    "    fig = go.Figure()\n",
    "\n",
    "    for t_id in selected_ids:\n",
    "        # Extract hits of the track and sort by radius r (from the center outward)\n",
    "        track_data = df_plot[df_plot['track_id'] == t_id].sort_values('r')\n",
    "        \n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=track_data['x'], y=track_data['y'], z=track_data['z'],\n",
    "            mode='lines+markers',\n",
    "            line=dict(width=4),\n",
    "            marker=dict(size=3, opacity=0.8),\n",
    "            name=f'Track {t_id} ({len(track_data)} hits)'\n",
    "        ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"Visualization of {len(selected_ids)} Reconstructed Trajectories\",\n",
    "        scene=dict(xaxis_title='X (mm)', yaxis_title='Y (mm)', zaxis_title='Z (mm)', aspectmode='data'),\n",
    "        margin=dict(l=0, r=0, b=0, t=40)\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "# Execution\n",
    "plot_final_tracks_3d(test_data, predicted_tracks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5db75de",
   "metadata": {},
   "source": [
    "We get very poor results. We can use a greedy solution instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6fdeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "def plot_all_reconstructed_tracks(test_data, hits_df, edge_probs, threshold=0.9, max_tracks=50):\n",
    "    \"\"\"\n",
    "    Reconstructs multiple tracks using the Greedy method and plots them in 3D.\n",
    "    \"\"\"\n",
    "    # Retrieve necessary data\n",
    "    src = test_data.edge_index[0].cpu().numpy()\n",
    "    dst = test_data.edge_index[1].cpu().numpy()\n",
    "    probs = edge_probs.cpu().detach().numpy() if torch.is_tensor(edge_probs) else edge_probs\n",
    "    pos = test_data.x[:, :3].cpu().numpy() * 100.0  # Convert back to mm\n",
    "    radii = test_data.x[:, 3].cpu().numpy()\n",
    "    \n",
    "    num_nodes = test_data.x.size(0)\n",
    "    used_hits = np.zeros(num_nodes, dtype=bool)\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Identify Seeds (Innermost layer)\n",
    "    min_layer = hits_df.global_layer.min()\n",
    "    seeds = np.where(hits_df.global_layer.values == min_layer)[0]\n",
    "    \n",
    "    tracks_found = 0\n",
    "    \n",
    "    # Reconstruction Loop\n",
    "    for seed in seeds:\n",
    "        if tracks_found >= max_tracks: break\n",
    "        if used_hits[seed]: continue\n",
    "        \n",
    "        track = [seed]\n",
    "        curr = seed\n",
    "        \n",
    "        # Build the track following the most probable path\n",
    "        for _ in range(15):\n",
    "            out_mask = (src == curr)\n",
    "            if not np.any(out_mask): break\n",
    "            \n",
    "            # Filters: Probability, Unused, and Increasing radius (outward)\n",
    "            candidates_dst = dst[out_mask]\n",
    "            candidates_probs = probs[out_mask]\n",
    "            \n",
    "            valid_mask = (candidates_probs > threshold) & \\\n",
    "                         (~used_hits[candidates_dst]) & \\\n",
    "                         (radii[candidates_dst] > radii[curr] + 0.0001)\n",
    "            \n",
    "            if not np.any(valid_mask): break\n",
    "            \n",
    "            # Select the best candidate and advance\n",
    "            best_idx = np.argmax(np.where(valid_mask, candidates_probs, -1))\n",
    "            curr = candidates_dst[best_idx]\n",
    "            track.append(curr)\n",
    "        \n",
    "        # If the track is valid, add it to the plot\n",
    "        if len(track) >= 3:\n",
    "            for h in track: used_hits[h] = True # Mark hits as used to prevent 'super tracks'\n",
    "            \n",
    "            track_pos = pos[track]\n",
    "            fig.add_trace(go.Scatter3d(\n",
    "                x=track_pos[:, 0], y=track_pos[:, 1], z=track_pos[:, 2],\n",
    "                mode='lines+markers',\n",
    "                line=dict(width=3),\n",
    "                marker=dict(size=2),\n",
    "                name=f'Track {tracks_found}'\n",
    "            ))\n",
    "            tracks_found += 1\n",
    "\n",
    "    # Plot configuration\n",
    "    fig.update_layout(\n",
    "        title=f\"Visualization of {tracks_found} Greedy Trajectories (Threshold {threshold})\",\n",
    "        scene=dict(xaxis_title='X (mm)', yaxis_title='Y (mm)', zaxis_title='Z (mm)', aspectmode='data'),\n",
    "        margin=dict(l=0, r=0, b=0, t=40)\n",
    "    )\n",
    "    fig.show()\n",
    "    print(f\"{tracks_found} individual trajectories have been plotted.\")\n",
    "\n",
    "plot_all_reconstructed_tracks(test_data, hits, probs, threshold=0.70, max_tracks=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caf74a0",
   "metadata": {},
   "source": [
    "Results seem to be more precise, but are still not perfect due to the imbalance of the data and other unexplored factors.\n",
    "\n",
    "**Better solutions are being explored.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
